{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Achm5gKVBgld"
      },
      "outputs": [],
      "source": [
        "# ETL: Extracción de datos de países más poblados desde Wikipedia\n",
        "# Autor: ETL Pipeline\n",
        "# Fecha: 2025-10-20\n",
        "\n",
        "# ============================================================================\n",
        "# INSTALACIÓN DE DEPENDENCIAS\n",
        "# ============================================================================\n",
        "!pip install selenium webdriver-manager pandas openpyxl -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# IMPORTS\n",
        "# ============================================================================\n",
        "import time\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Optional\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "import re"
      ],
      "metadata": {
        "id": "YqYfjZ8YCIaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " ============================================================================\n",
        "# CONFIGURACIÓN DE LOGGING\n",
        "# ============================================================================\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger('ETL_Pipeline')"
      ],
      "metadata": {
        "id": "ZjmJGMTNCTnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CLASE ETL PRINCIPAL\n",
        "# ============================================================================\n",
        "class WikipediaCountriesETL:\n",
        "    \"\"\"\n",
        "    ETL Pipeline para extraer, transformar y cargar datos de países desde Wikipedia.\n",
        "\n",
        "    Implementa el patrón ETL con las siguientes características:\n",
        "    - Extracción mediante Selenium\n",
        "    - Transformación con Pandas\n",
        "    - Validación de datos\n",
        "    - Manejo robusto de errores\n",
        "    - Logging comprehensivo\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, url: str):\n",
        "        \"\"\"\n",
        "        Inicializa el pipeline ETL.\n",
        "\n",
        "        Args:\n",
        "            url: URL de Wikipedia a extraer\n",
        "        \"\"\"\n",
        "        self.url = url\n",
        "        self.driver = None\n",
        "        self.raw_data = None\n",
        "        self.transformed_data = None\n",
        "        self.metadata = {\n",
        "            'extraction_time': None,\n",
        "            'total_records': 0,\n",
        "            'valid_records': 0,\n",
        "            'invalid_records': 0,\n",
        "            'transformation_time': None\n",
        "        }\n",
        "\n",
        "    def _setup_driver(self) -> webdriver.Chrome:\n",
        "        \"\"\"\n",
        "        Configura el driver de Selenium con opciones optimizadas y seguras.\n",
        "\n",
        "        Returns:\n",
        "            WebDriver configurado\n",
        "        \"\"\"\n",
        "        logger.info(\"Configurando Selenium WebDriver...\")\n",
        "\n",
        "        chrome_options = Options()\n",
        "        # Seguridad y performance\n",
        "        chrome_options.add_argument('--headless')  # Modo sin interfaz gráfica\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "        chrome_options.add_argument('--disable-gpu')\n",
        "        chrome_options.add_argument('--window-size=1920,1080')\n",
        "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "\n",
        "        # Seguridad adicional\n",
        "        chrome_options.add_argument('--disable-extensions')\n",
        "        chrome_options.add_argument('--disable-plugins')\n",
        "        chrome_options.add_argument('--disable-images')  # Más rápido y seguro\n",
        "        chrome_options.add_argument('--disable-javascript')  # Solo necesitamos HTML estático\n",
        "        chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
        "\n",
        "        # User agent legítimo\n",
        "        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
        "\n",
        "        service = Service(ChromeDriverManager().install())\n",
        "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "        # Timeout de seguridad\n",
        "        driver.set_page_load_timeout(30)\n",
        "\n",
        "        logger.info(\"WebDriver configurado exitosamente\")\n",
        "        return driver\n",
        "\n",
        "    # ========================================================================\n",
        "    # EXTRACT\n",
        "    # ========================================================================\n",
        "    def extract(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Extrae datos de la tabla de Wikipedia usando Selenium.\n",
        "\n",
        "        Returns:\n",
        "            Lista de diccionarios con los datos extraídos\n",
        "        \"\"\"\n",
        "        logger.info(f\"Iniciando extracción desde: {self.url}\")\n",
        "\n",
        "        # Validación de URL por seguridad\n",
        "        if not self.url.startswith('https://'):\n",
        "            raise ValueError(\"Solo se permiten URLs HTTPS por seguridad\")\n",
        "\n",
        "        allowed_domains = ['wikipedia.org', 'en.wikipedia.org']\n",
        "        from urllib.parse import urlparse\n",
        "        domain = urlparse(self.url).netloc\n",
        "        if not any(allowed in domain for allowed in allowed_domains):\n",
        "            raise ValueError(f\"Dominio no permitido: {domain}. Solo Wikipedia es permitida.\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            self.driver = self._setup_driver()\n",
        "\n",
        "            # Rate limiting: respetar los servidores\n",
        "            time.sleep(2)\n",
        "\n",
        "            self.driver.get(self.url)\n",
        "\n",
        "            # Esperar a que la tabla esté presente\n",
        "            logger.info(\"Esperando carga de tabla...\")\n",
        "            wait = WebDriverWait(self.driver, 15)\n",
        "            table = wait.until(\n",
        "                EC.presence_of_element_located((By.CLASS_NAME, \"wikitable\"))\n",
        "            )\n",
        "\n",
        "            logger.info(\"Tabla encontrada, extrayendo datos...\")\n",
        "\n",
        "            # Extraer headers\n",
        "            headers_elements = table.find_elements(By.TAG_NAME, \"th\")\n",
        "            headers = [header.text.strip() for header in headers_elements[:7]]\n",
        "            logger.info(f\"Headers encontrados: {headers}\")\n",
        "\n",
        "            # Extraer filas\n",
        "            rows = table.find_elements(By.TAG_NAME, \"tr\")[1:]  # Saltar header\n",
        "            raw_data = []\n",
        "\n",
        "            for idx, row in enumerate(rows[:50], 1):  # Primeros 50 países\n",
        "                try:\n",
        "                    cells = row.find_elements(By.TAG_NAME, \"td\")\n",
        "                    if len(cells) >= 6:\n",
        "                        row_data = {\n",
        "                            'rank': self._safe_extract(cells[0]),\n",
        "                            'country': self._safe_extract(cells[1]),\n",
        "                            'population': self._safe_extract(cells[2]),\n",
        "                            'percentage': self._safe_extract(cells[3]),\n",
        "                            'date': self._safe_extract(cells[4]),\n",
        "                            'source': self._safe_extract(cells[5])\n",
        "                        }\n",
        "                        raw_data.append(row_data)\n",
        "\n",
        "                        if idx % 10 == 0:\n",
        "                            logger.info(f\"Extraídos {idx} registros...\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error en fila {idx}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            self.raw_data = raw_data\n",
        "            self.metadata['extraction_time'] = time.time() - start_time\n",
        "            self.metadata['total_records'] = len(raw_data)\n",
        "\n",
        "            logger.info(f\"Extracción completada: {len(raw_data)} registros en {self.metadata['extraction_time']:.2f}s\")\n",
        "            return raw_data\n",
        "\n",
        "        except TimeoutException:\n",
        "            logger.error(\"Timeout esperando la carga de la página\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error durante extracción: {str(e)}\")\n",
        "            raise\n",
        "        finally:\n",
        "            if self.driver:\n",
        "                self.driver.quit()\n",
        "                logger.info(\"WebDriver cerrado\")\n",
        "\n",
        "    def _safe_extract(self, element) -> str:\n",
        "        \"\"\"\n",
        "        Extrae texto de forma segura manejando excepciones.\n",
        "\n",
        "        Args:\n",
        "            element: Elemento web a extraer\n",
        "\n",
        "        Returns:\n",
        "            Texto del elemento o cadena vacía\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return element.text.strip()\n",
        "        except:\n",
        "            return \"\"\n",
        "\n",
        "    # ========================================================================\n",
        "    # TRANSFORM\n",
        "    # ========================================================================\n",
        "    def transform(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Transforma los datos extraídos aplicando limpieza y enriquecimiento.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame transformado\n",
        "        \"\"\"\n",
        "        logger.info(\"Iniciando transformación de datos...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        if not self.raw_data:\n",
        "            raise ValueError(\"No hay datos para transformar. Ejecute extract() primero.\")\n",
        "\n",
        "        # Crear DataFrame\n",
        "        df = pd.DataFrame(self.raw_data)\n",
        "        initial_count = len(df)\n",
        "        logger.info(f\"DataFrame creado con {initial_count} registros\")\n",
        "\n",
        "        # 1. Limpieza de datos\n",
        "        df = self._clean_data(df)\n",
        "\n",
        "        # 2. Conversión de tipos\n",
        "        df = self._convert_types(df)\n",
        "\n",
        "        # 3. Validación de datos\n",
        "        df = self._validate_data(df)\n",
        "\n",
        "        # 4. Enriquecimiento\n",
        "        df = self._enrich_data(df)\n",
        "\n",
        "        # 5. Crear columnas calculadas\n",
        "        df = self._calculate_metrics(df)\n",
        "\n",
        "        self.transformed_data = df\n",
        "        self.metadata['transformation_time'] = time.time() - start_time\n",
        "        self.metadata['valid_records'] = len(df)\n",
        "        self.metadata['invalid_records'] = initial_count - len(df)\n",
        "\n",
        "        logger.info(f\"Transformación completada: {len(df)} registros válidos en {self.metadata['transformation_time']:.2f}s\")\n",
        "        return df\n",
        "\n",
        "    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Limpia los datos removiendo caracteres especiales y normalizando.\"\"\"\n",
        "        logger.info(\"Limpiando datos...\")\n",
        "\n",
        "        # Limpiar población: remover comas, corchetes, notas\n",
        "        df['population_clean'] = df['population'].apply(self._clean_population)\n",
        "\n",
        "        # Limpiar porcentaje\n",
        "        df['percentage_clean'] = df['percentage'].str.replace('%', '').str.strip()\n",
        "\n",
        "        # Limpiar país (remover notas al pie)\n",
        "        df['country_clean'] = df['country'].apply(lambda x: re.split(r'\\[|\\(', x)[0].strip())\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _clean_population(self, pop_str: str) -> str:\n",
        "        \"\"\"Limpia el string de población.\"\"\"\n",
        "        if not isinstance(pop_str, str):\n",
        "            return \"\"\n",
        "        # Remover todo excepto dígitos y comas\n",
        "        cleaned = re.sub(r'[^\\d,]', '', pop_str)\n",
        "        # Remover comas\n",
        "        cleaned = cleaned.replace(',', '')\n",
        "        return cleaned\n",
        "\n",
        "    def _convert_types(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Convierte columnas a tipos apropiados.\"\"\"\n",
        "        logger.info(\"Convirtiendo tipos de datos...\")\n",
        "\n",
        "        # Convertir población a numérico\n",
        "        df['population_numeric'] = pd.to_numeric(df['population_clean'], errors='coerce')\n",
        "\n",
        "        # Convertir porcentaje a numérico\n",
        "        df['percentage_numeric'] = pd.to_numeric(df['percentage_clean'], errors='coerce')\n",
        "\n",
        "        # Convertir rank a numérico\n",
        "        df['rank_numeric'] = pd.to_numeric(df['rank'], errors='coerce')\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _validate_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Valida y filtra datos inválidos.\"\"\"\n",
        "        logger.info(\"Validando datos...\")\n",
        "\n",
        "        # Filtrar registros sin población válida\n",
        "        df_valid = df[df['population_numeric'].notna()].copy()\n",
        "        removed = len(df) - len(df_valid)\n",
        "\n",
        "        if removed > 0:\n",
        "            logger.warning(f\"Removidos {removed} registros con población inválida\")\n",
        "\n",
        "        # Filtrar población > 0\n",
        "        df_valid = df_valid[df_valid['population_numeric'] > 0]\n",
        "\n",
        "        return df_valid\n",
        "\n",
        "    def _enrich_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Enriquece los datos con información adicional.\"\"\"\n",
        "        logger.info(\"Enriqueciendo datos...\")\n",
        "\n",
        "        # Categorizar países por población\n",
        "        df['population_category'] = pd.cut(\n",
        "            df['population_numeric'],\n",
        "            bins=[0, 10_000_000, 50_000_000, 100_000_000, 500_000_000, float('inf')],\n",
        "            labels=['Pequeño', 'Mediano', 'Grande', 'Muy Grande', 'Mega Poblado']\n",
        "        )\n",
        "\n",
        "        # Añadir timestamp de procesamiento\n",
        "        df['processed_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "        # Año de extracción\n",
        "        df['extraction_year'] = datetime.now().year\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _calculate_metrics(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Calcula métricas adicionales.\"\"\"\n",
        "        logger.info(\"Calculando métricas...\")\n",
        "\n",
        "        # Población en millones\n",
        "        df['population_millions'] = (df['population_numeric'] / 1_000_000).round(2)\n",
        "\n",
        "        # Porcentaje acumulado\n",
        "        df['cumulative_percentage'] = df['percentage_numeric'].cumsum()\n",
        "\n",
        "        # Ranking por densidad relativa\n",
        "        df['relative_rank'] = df['population_numeric'].rank(ascending=False, method='dense')\n",
        "\n",
        "        # Diferencia con el país anterior\n",
        "        df['population_diff'] = df['population_numeric'].diff().abs()\n",
        "        df['population_diff_millions'] = (df['population_diff'] / 1_000_000).round(2)\n",
        "\n",
        "        return df\n",
        "\n",
        "    # ========================================================================\n",
        "    # LOAD & ANALYSIS\n",
        "    # ========================================================================\n",
        "    def load(self, output_format: str = 'csv', filename: str = 'countries_data'):\n",
        "        \"\"\"\n",
        "        Carga los datos transformados en el formato especificado.\n",
        "\n",
        "        Args:\n",
        "            output_format: Formato de salida ('csv', 'excel', 'json')\n",
        "            filename: Nombre del archivo de salida\n",
        "        \"\"\"\n",
        "        if self.transformed_data is None:\n",
        "            raise ValueError(\"No hay datos transformados. Ejecute transform() primero.\")\n",
        "\n",
        "        logger.info(f\"Cargando datos en formato {output_format}...\")\n",
        "\n",
        "        if output_format == 'csv':\n",
        "            filepath = f\"{filename}.csv\"\n",
        "            self.transformed_data.to_csv(filepath, index=False, encoding='utf-8-sig')\n",
        "        elif output_format == 'excel':\n",
        "            filepath = f\"{filename}.xlsx\"\n",
        "            self.transformed_data.to_excel(filepath, index=False, engine='openpyxl')\n",
        "        elif output_format == 'json':\n",
        "            filepath = f\"{filename}.json\"\n",
        "            self.transformed_data.to_json(filepath, orient='records', indent=2, force_ascii=False)\n",
        "        else:\n",
        "            raise ValueError(f\"Formato no soportado: {output_format}\")\n",
        "\n",
        "        logger.info(f\"Datos guardados exitosamente en: {filepath}\")\n",
        "        return filepath\n",
        "\n",
        "    def get_summary_statistics(self) -> pd.DataFrame:\n",
        "        \"\"\"Retorna estadísticas descriptivas del dataset.\"\"\"\n",
        "        if self.transformed_data is None:\n",
        "            raise ValueError(\"No hay datos transformados disponibles.\")\n",
        "\n",
        "        stats = self.transformed_data[['population_numeric', 'percentage_numeric', 'population_millions']].describe()\n",
        "        return stats\n",
        "\n",
        "    def get_metadata(self) -> Dict:\n",
        "        \"\"\"Retorna metadata del proceso ETL.\"\"\"\n",
        "        return self.metadata\n",
        "\n",
        "    def display_top_countries(self, n: int = 10):\n",
        "        \"\"\"Muestra los top N países con formato mejorado.\"\"\"\n",
        "        if self.transformed_data is None:\n",
        "            raise ValueError(\"No hay datos transformados disponibles.\")\n",
        "\n",
        "        top_countries = self.transformed_data.head(n)[\n",
        "            ['rank_numeric', 'country_clean', 'population_millions',\n",
        "             'percentage_numeric', 'population_category']\n",
        "        ].copy()\n",
        "\n",
        "        top_countries.columns = ['Ranking', 'País', 'Población (M)', '% Mundial', 'Categoría']\n",
        "\n",
        "        return top_countries"
      ],
      "metadata": {
        "id": "IGMeYmycCWzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# EJECUCIÓN DEL ETL\n",
        "# ============================================================================\n",
        "def main():\n",
        "    \"\"\"Función principal para ejecutar el pipeline ETL.\"\"\"\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"ETL PIPELINE - PAÍSES MÁS POBLADOS DEL MUNDO\")\n",
        "    print(\"=\" * 80)\n",
        "    print()\n",
        "\n",
        "    # URL de Wikipedia\n",
        "    url = \"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population\"\n",
        "\n",
        "    # Crear instancia del ETL\n",
        "    etl = WikipediaCountriesETL(url)\n",
        "\n",
        "    try:\n",
        "        # EXTRACT\n",
        "        print(\"\\n📥 FASE 1: EXTRACCIÓN\")\n",
        "        print(\"-\" * 80)\n",
        "        raw_data = etl.extract()\n",
        "        print(f\"✅ Extraídos {len(raw_data)} registros\")\n",
        "\n",
        "        # TRANSFORM\n",
        "        print(\"\\n🔄 FASE 2: TRANSFORMACIÓN\")\n",
        "        print(\"-\" * 80)\n",
        "        transformed_df = etl.transform()\n",
        "        print(f\"✅ Transformados {len(transformed_df)} registros válidos\")\n",
        "\n",
        "        # LOAD\n",
        "        print(\"\\n💾 FASE 3: CARGA\")\n",
        "        print(\"-\" * 80)\n",
        "        csv_file = etl.load('csv', 'countries_population')\n",
        "        excel_file = etl.load('excel', 'countries_population')\n",
        "        print(f\"✅ Datos guardados en: {csv_file} y {excel_file}\")\n",
        "\n",
        "        # ANÁLISIS Y RESULTADOS\n",
        "        print(\"\\n📊 RESULTADOS DEL ETL\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Metadata\n",
        "        metadata = etl.get_metadata()\n",
        "        print(f\"\\n⏱️  Tiempo de extracción: {metadata['extraction_time']:.2f}s\")\n",
        "        print(f\"⏱️  Tiempo de transformación: {metadata['transformation_time']:.2f}s\")\n",
        "        print(f\"📈 Total de registros: {metadata['total_records']}\")\n",
        "        print(f\"✅ Registros válidos: {metadata['valid_records']}\")\n",
        "        print(f\"❌ Registros inválidos: {metadata['invalid_records']}\")\n",
        "\n",
        "        # Top 10 países\n",
        "        print(\"\\n🌍 TOP 10 PAÍSES MÁS POBLADOS\")\n",
        "        print(\"-\" * 80)\n",
        "        top_10 = etl.display_top_countries(10)\n",
        "        print(top_10.to_string(index=False))\n",
        "\n",
        "        # Estadísticas descriptivas\n",
        "        print(\"\\n📈 ESTADÍSTICAS DESCRIPTIVAS\")\n",
        "        print(\"-\" * 80)\n",
        "        stats = etl.get_summary_statistics()\n",
        "        print(stats)\n",
        "\n",
        "        # Análisis adicional\n",
        "        print(\"\\n🔍 ANÁLISIS ADICIONAL\")\n",
        "        print(\"-\" * 80)\n",
        "        df = etl.transformed_data\n",
        "\n",
        "        print(f\"• Población total (top 50): {df['population_numeric'].sum():,.0f} habitantes\")\n",
        "        print(f\"• Población promedio: {df['population_numeric'].mean():,.0f} habitantes\")\n",
        "        print(f\"• País más poblado: {df.iloc[0]['country_clean']} ({df.iloc[0]['population_millions']:.2f}M)\")\n",
        "        print(f\"• Distribución por categoría:\")\n",
        "        print(df['population_category'].value_counts().to_string())\n",
        "\n",
        "        # Porcentaje acumulado\n",
        "        top_10_percentage = df.head(10)['percentage_numeric'].sum()\n",
        "        print(f\"\\n• Top 10 países representan: {top_10_percentage:.2f}% de la población mundial\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"✅ ETL COMPLETADO EXITOSAMENTE\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        return etl\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en el pipeline ETL: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "EmvGuksfC1Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# EJECUTAR\n",
        "# ============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    etl_pipeline = main()"
      ],
      "metadata": {
        "id": "sqxN4S_IC24j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}