{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Achm5gKVBgld"
      },
      "outputs": [],
      "source": [
        "# ETL: Extracci√≥n de datos de pa√≠ses m√°s poblados desde Wikipedia\n",
        "# Autor: ETL Pipeline\n",
        "# Fecha: 2025-10-20\n",
        "\n",
        "# ============================================================================\n",
        "# INSTALACI√ìN DE DEPENDENCIAS\n",
        "# ============================================================================\n",
        "!pip install selenium webdriver-manager pandas openpyxl -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# IMPORTS\n",
        "# ============================================================================\n",
        "import time\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Optional\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "import re"
      ],
      "metadata": {
        "id": "YqYfjZ8YCIaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " ============================================================================\n",
        "# CONFIGURACI√ìN DE LOGGING\n",
        "# ============================================================================\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger('ETL_Pipeline')"
      ],
      "metadata": {
        "id": "ZjmJGMTNCTnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CLASE ETL PRINCIPAL\n",
        "# ============================================================================\n",
        "class WikipediaCountriesETL:\n",
        "    \"\"\"\n",
        "    ETL Pipeline para extraer, transformar y cargar datos de pa√≠ses desde Wikipedia.\n",
        "\n",
        "    Implementa el patr√≥n ETL con las siguientes caracter√≠sticas:\n",
        "    - Extracci√≥n mediante Selenium\n",
        "    - Transformaci√≥n con Pandas\n",
        "    - Validaci√≥n de datos\n",
        "    - Manejo robusto de errores\n",
        "    - Logging comprehensivo\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, url: str):\n",
        "        \"\"\"\n",
        "        Inicializa el pipeline ETL.\n",
        "\n",
        "        Args:\n",
        "            url: URL de Wikipedia a extraer\n",
        "        \"\"\"\n",
        "        self.url = url\n",
        "        self.driver = None\n",
        "        self.raw_data = None\n",
        "        self.transformed_data = None\n",
        "        self.metadata = {\n",
        "            'extraction_time': None,\n",
        "            'total_records': 0,\n",
        "            'valid_records': 0,\n",
        "            'invalid_records': 0,\n",
        "            'transformation_time': None\n",
        "        }\n",
        "\n",
        "    def _setup_driver(self) -> webdriver.Chrome:\n",
        "        \"\"\"\n",
        "        Configura el driver de Selenium con opciones optimizadas y seguras.\n",
        "\n",
        "        Returns:\n",
        "            WebDriver configurado\n",
        "        \"\"\"\n",
        "        logger.info(\"Configurando Selenium WebDriver...\")\n",
        "\n",
        "        chrome_options = Options()\n",
        "        # Seguridad y performance\n",
        "        chrome_options.add_argument('--headless')  # Modo sin interfaz gr√°fica\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "        chrome_options.add_argument('--disable-gpu')\n",
        "        chrome_options.add_argument('--window-size=1920,1080')\n",
        "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "\n",
        "        # Seguridad adicional\n",
        "        chrome_options.add_argument('--disable-extensions')\n",
        "        chrome_options.add_argument('--disable-plugins')\n",
        "        chrome_options.add_argument('--disable-images')  # M√°s r√°pido y seguro\n",
        "        chrome_options.add_argument('--disable-javascript')  # Solo necesitamos HTML est√°tico\n",
        "        chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
        "\n",
        "        # User agent leg√≠timo\n",
        "        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
        "\n",
        "        service = Service(ChromeDriverManager().install())\n",
        "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "        # Timeout de seguridad\n",
        "        driver.set_page_load_timeout(30)\n",
        "\n",
        "        logger.info(\"WebDriver configurado exitosamente\")\n",
        "        return driver\n",
        "\n",
        "    # ========================================================================\n",
        "    # EXTRACT\n",
        "    # ========================================================================\n",
        "    def extract(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Extrae datos de la tabla de Wikipedia usando Selenium.\n",
        "\n",
        "        Returns:\n",
        "            Lista de diccionarios con los datos extra√≠dos\n",
        "        \"\"\"\n",
        "        logger.info(f\"Iniciando extracci√≥n desde: {self.url}\")\n",
        "\n",
        "        # Validaci√≥n de URL por seguridad\n",
        "        if not self.url.startswith('https://'):\n",
        "            raise ValueError(\"Solo se permiten URLs HTTPS por seguridad\")\n",
        "\n",
        "        allowed_domains = ['wikipedia.org', 'en.wikipedia.org']\n",
        "        from urllib.parse import urlparse\n",
        "        domain = urlparse(self.url).netloc\n",
        "        if not any(allowed in domain for allowed in allowed_domains):\n",
        "            raise ValueError(f\"Dominio no permitido: {domain}. Solo Wikipedia es permitida.\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            self.driver = self._setup_driver()\n",
        "\n",
        "            # Rate limiting: respetar los servidores\n",
        "            time.sleep(2)\n",
        "\n",
        "            self.driver.get(self.url)\n",
        "\n",
        "            # Esperar a que la tabla est√© presente\n",
        "            logger.info(\"Esperando carga de tabla...\")\n",
        "            wait = WebDriverWait(self.driver, 15)\n",
        "            table = wait.until(\n",
        "                EC.presence_of_element_located((By.CLASS_NAME, \"wikitable\"))\n",
        "            )\n",
        "\n",
        "            logger.info(\"Tabla encontrada, extrayendo datos...\")\n",
        "\n",
        "            # Extraer headers\n",
        "            headers_elements = table.find_elements(By.TAG_NAME, \"th\")\n",
        "            headers = [header.text.strip() for header in headers_elements[:7]]\n",
        "            logger.info(f\"Headers encontrados: {headers}\")\n",
        "\n",
        "            # Extraer filas\n",
        "            rows = table.find_elements(By.TAG_NAME, \"tr\")[1:]  # Saltar header\n",
        "            raw_data = []\n",
        "\n",
        "            for idx, row in enumerate(rows[:50], 1):  # Primeros 50 pa√≠ses\n",
        "                try:\n",
        "                    cells = row.find_elements(By.TAG_NAME, \"td\")\n",
        "                    if len(cells) >= 6:\n",
        "                        row_data = {\n",
        "                            'rank': self._safe_extract(cells[0]),\n",
        "                            'country': self._safe_extract(cells[1]),\n",
        "                            'population': self._safe_extract(cells[2]),\n",
        "                            'percentage': self._safe_extract(cells[3]),\n",
        "                            'date': self._safe_extract(cells[4]),\n",
        "                            'source': self._safe_extract(cells[5])\n",
        "                        }\n",
        "                        raw_data.append(row_data)\n",
        "\n",
        "                        if idx % 10 == 0:\n",
        "                            logger.info(f\"Extra√≠dos {idx} registros...\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error en fila {idx}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            self.raw_data = raw_data\n",
        "            self.metadata['extraction_time'] = time.time() - start_time\n",
        "            self.metadata['total_records'] = len(raw_data)\n",
        "\n",
        "            logger.info(f\"Extracci√≥n completada: {len(raw_data)} registros en {self.metadata['extraction_time']:.2f}s\")\n",
        "            return raw_data\n",
        "\n",
        "        except TimeoutException:\n",
        "            logger.error(\"Timeout esperando la carga de la p√°gina\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error durante extracci√≥n: {str(e)}\")\n",
        "            raise\n",
        "        finally:\n",
        "            if self.driver:\n",
        "                self.driver.quit()\n",
        "                logger.info(\"WebDriver cerrado\")\n",
        "\n",
        "    def _safe_extract(self, element) -> str:\n",
        "        \"\"\"\n",
        "        Extrae texto de forma segura manejando excepciones.\n",
        "\n",
        "        Args:\n",
        "            element: Elemento web a extraer\n",
        "\n",
        "        Returns:\n",
        "            Texto del elemento o cadena vac√≠a\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return element.text.strip()\n",
        "        except:\n",
        "            return \"\"\n",
        "\n",
        "    # ========================================================================\n",
        "    # TRANSFORM\n",
        "    # ========================================================================\n",
        "    def transform(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Transforma los datos extra√≠dos aplicando limpieza y enriquecimiento.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame transformado\n",
        "        \"\"\"\n",
        "        logger.info(\"Iniciando transformaci√≥n de datos...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        if not self.raw_data:\n",
        "            raise ValueError(\"No hay datos para transformar. Ejecute extract() primero.\")\n",
        "\n",
        "        # Crear DataFrame\n",
        "        df = pd.DataFrame(self.raw_data)\n",
        "        initial_count = len(df)\n",
        "        logger.info(f\"DataFrame creado con {initial_count} registros\")\n",
        "\n",
        "        # 1. Limpieza de datos\n",
        "        df = self._clean_data(df)\n",
        "\n",
        "        # 2. Conversi√≥n de tipos\n",
        "        df = self._convert_types(df)\n",
        "\n",
        "        # 3. Validaci√≥n de datos\n",
        "        df = self._validate_data(df)\n",
        "\n",
        "        # 4. Enriquecimiento\n",
        "        df = self._enrich_data(df)\n",
        "\n",
        "        # 5. Crear columnas calculadas\n",
        "        df = self._calculate_metrics(df)\n",
        "\n",
        "        self.transformed_data = df\n",
        "        self.metadata['transformation_time'] = time.time() - start_time\n",
        "        self.metadata['valid_records'] = len(df)\n",
        "        self.metadata['invalid_records'] = initial_count - len(df)\n",
        "\n",
        "        logger.info(f\"Transformaci√≥n completada: {len(df)} registros v√°lidos en {self.metadata['transformation_time']:.2f}s\")\n",
        "        return df\n",
        "\n",
        "    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Limpia los datos removiendo caracteres especiales y normalizando.\"\"\"\n",
        "        logger.info(\"Limpiando datos...\")\n",
        "\n",
        "        # Limpiar poblaci√≥n: remover comas, corchetes, notas\n",
        "        df['population_clean'] = df['population'].apply(self._clean_population)\n",
        "\n",
        "        # Limpiar porcentaje\n",
        "        df['percentage_clean'] = df['percentage'].str.replace('%', '').str.strip()\n",
        "\n",
        "        # Limpiar pa√≠s (remover notas al pie)\n",
        "        df['country_clean'] = df['country'].apply(lambda x: re.split(r'\\[|\\(', x)[0].strip())\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _clean_population(self, pop_str: str) -> str:\n",
        "        \"\"\"Limpia el string de poblaci√≥n.\"\"\"\n",
        "        if not isinstance(pop_str, str):\n",
        "            return \"\"\n",
        "        # Remover todo excepto d√≠gitos y comas\n",
        "        cleaned = re.sub(r'[^\\d,]', '', pop_str)\n",
        "        # Remover comas\n",
        "        cleaned = cleaned.replace(',', '')\n",
        "        return cleaned\n",
        "\n",
        "    def _convert_types(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Convierte columnas a tipos apropiados.\"\"\"\n",
        "        logger.info(\"Convirtiendo tipos de datos...\")\n",
        "\n",
        "        # Convertir poblaci√≥n a num√©rico\n",
        "        df['population_numeric'] = pd.to_numeric(df['population_clean'], errors='coerce')\n",
        "\n",
        "        # Convertir porcentaje a num√©rico\n",
        "        df['percentage_numeric'] = pd.to_numeric(df['percentage_clean'], errors='coerce')\n",
        "\n",
        "        # Convertir rank a num√©rico\n",
        "        df['rank_numeric'] = pd.to_numeric(df['rank'], errors='coerce')\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _validate_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Valida y filtra datos inv√°lidos.\"\"\"\n",
        "        logger.info(\"Validando datos...\")\n",
        "\n",
        "        # Filtrar registros sin poblaci√≥n v√°lida\n",
        "        df_valid = df[df['population_numeric'].notna()].copy()\n",
        "        removed = len(df) - len(df_valid)\n",
        "\n",
        "        if removed > 0:\n",
        "            logger.warning(f\"Removidos {removed} registros con poblaci√≥n inv√°lida\")\n",
        "\n",
        "        # Filtrar poblaci√≥n > 0\n",
        "        df_valid = df_valid[df_valid['population_numeric'] > 0]\n",
        "\n",
        "        return df_valid\n",
        "\n",
        "    def _enrich_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Enriquece los datos con informaci√≥n adicional.\"\"\"\n",
        "        logger.info(\"Enriqueciendo datos...\")\n",
        "\n",
        "        # Categorizar pa√≠ses por poblaci√≥n\n",
        "        df['population_category'] = pd.cut(\n",
        "            df['population_numeric'],\n",
        "            bins=[0, 10_000_000, 50_000_000, 100_000_000, 500_000_000, float('inf')],\n",
        "            labels=['Peque√±o', 'Mediano', 'Grande', 'Muy Grande', 'Mega Poblado']\n",
        "        )\n",
        "\n",
        "        # A√±adir timestamp de procesamiento\n",
        "        df['processed_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "        # A√±o de extracci√≥n\n",
        "        df['extraction_year'] = datetime.now().year\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _calculate_metrics(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Calcula m√©tricas adicionales.\"\"\"\n",
        "        logger.info(\"Calculando m√©tricas...\")\n",
        "\n",
        "        # Poblaci√≥n en millones\n",
        "        df['population_millions'] = (df['population_numeric'] / 1_000_000).round(2)\n",
        "\n",
        "        # Porcentaje acumulado\n",
        "        df['cumulative_percentage'] = df['percentage_numeric'].cumsum()\n",
        "\n",
        "        # Ranking por densidad relativa\n",
        "        df['relative_rank'] = df['population_numeric'].rank(ascending=False, method='dense')\n",
        "\n",
        "        # Diferencia con el pa√≠s anterior\n",
        "        df['population_diff'] = df['population_numeric'].diff().abs()\n",
        "        df['population_diff_millions'] = (df['population_diff'] / 1_000_000).round(2)\n",
        "\n",
        "        return df\n",
        "\n",
        "    # ========================================================================\n",
        "    # LOAD & ANALYSIS\n",
        "    # ========================================================================\n",
        "    def load(self, output_format: str = 'csv', filename: str = 'countries_data'):\n",
        "        \"\"\"\n",
        "        Carga los datos transformados en el formato especificado.\n",
        "\n",
        "        Args:\n",
        "            output_format: Formato de salida ('csv', 'excel', 'json')\n",
        "            filename: Nombre del archivo de salida\n",
        "        \"\"\"\n",
        "        if self.transformed_data is None:\n",
        "            raise ValueError(\"No hay datos transformados. Ejecute transform() primero.\")\n",
        "\n",
        "        logger.info(f\"Cargando datos en formato {output_format}...\")\n",
        "\n",
        "        if output_format == 'csv':\n",
        "            filepath = f\"{filename}.csv\"\n",
        "            self.transformed_data.to_csv(filepath, index=False, encoding='utf-8-sig')\n",
        "        elif output_format == 'excel':\n",
        "            filepath = f\"{filename}.xlsx\"\n",
        "            self.transformed_data.to_excel(filepath, index=False, engine='openpyxl')\n",
        "        elif output_format == 'json':\n",
        "            filepath = f\"{filename}.json\"\n",
        "            self.transformed_data.to_json(filepath, orient='records', indent=2, force_ascii=False)\n",
        "        else:\n",
        "            raise ValueError(f\"Formato no soportado: {output_format}\")\n",
        "\n",
        "        logger.info(f\"Datos guardados exitosamente en: {filepath}\")\n",
        "        return filepath\n",
        "\n",
        "    def get_summary_statistics(self) -> pd.DataFrame:\n",
        "        \"\"\"Retorna estad√≠sticas descriptivas del dataset.\"\"\"\n",
        "        if self.transformed_data is None:\n",
        "            raise ValueError(\"No hay datos transformados disponibles.\")\n",
        "\n",
        "        stats = self.transformed_data[['population_numeric', 'percentage_numeric', 'population_millions']].describe()\n",
        "        return stats\n",
        "\n",
        "    def get_metadata(self) -> Dict:\n",
        "        \"\"\"Retorna metadata del proceso ETL.\"\"\"\n",
        "        return self.metadata\n",
        "\n",
        "    def display_top_countries(self, n: int = 10):\n",
        "        \"\"\"Muestra los top N pa√≠ses con formato mejorado.\"\"\"\n",
        "        if self.transformed_data is None:\n",
        "            raise ValueError(\"No hay datos transformados disponibles.\")\n",
        "\n",
        "        top_countries = self.transformed_data.head(n)[\n",
        "            ['rank_numeric', 'country_clean', 'population_millions',\n",
        "             'percentage_numeric', 'population_category']\n",
        "        ].copy()\n",
        "\n",
        "        top_countries.columns = ['Ranking', 'Pa√≠s', 'Poblaci√≥n (M)', '% Mundial', 'Categor√≠a']\n",
        "\n",
        "        return top_countries"
      ],
      "metadata": {
        "id": "IGMeYmycCWzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# EJECUCI√ìN DEL ETL\n",
        "# ============================================================================\n",
        "def main():\n",
        "    \"\"\"Funci√≥n principal para ejecutar el pipeline ETL.\"\"\"\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"ETL PIPELINE - PA√çSES M√ÅS POBLADOS DEL MUNDO\")\n",
        "    print(\"=\" * 80)\n",
        "    print()\n",
        "\n",
        "    # URL de Wikipedia\n",
        "    url = \"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population\"\n",
        "\n",
        "    # Crear instancia del ETL\n",
        "    etl = WikipediaCountriesETL(url)\n",
        "\n",
        "    try:\n",
        "        # EXTRACT\n",
        "        print(\"\\nüì• FASE 1: EXTRACCI√ìN\")\n",
        "        print(\"-\" * 80)\n",
        "        raw_data = etl.extract()\n",
        "        print(f\"‚úÖ Extra√≠dos {len(raw_data)} registros\")\n",
        "\n",
        "        # TRANSFORM\n",
        "        print(\"\\nüîÑ FASE 2: TRANSFORMACI√ìN\")\n",
        "        print(\"-\" * 80)\n",
        "        transformed_df = etl.transform()\n",
        "        print(f\"‚úÖ Transformados {len(transformed_df)} registros v√°lidos\")\n",
        "\n",
        "        # LOAD\n",
        "        print(\"\\nüíæ FASE 3: CARGA\")\n",
        "        print(\"-\" * 80)\n",
        "        csv_file = etl.load('csv', 'countries_population')\n",
        "        excel_file = etl.load('excel', 'countries_population')\n",
        "        print(f\"‚úÖ Datos guardados en: {csv_file} y {excel_file}\")\n",
        "\n",
        "        # AN√ÅLISIS Y RESULTADOS\n",
        "        print(\"\\nüìä RESULTADOS DEL ETL\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Metadata\n",
        "        metadata = etl.get_metadata()\n",
        "        print(f\"\\n‚è±Ô∏è  Tiempo de extracci√≥n: {metadata['extraction_time']:.2f}s\")\n",
        "        print(f\"‚è±Ô∏è  Tiempo de transformaci√≥n: {metadata['transformation_time']:.2f}s\")\n",
        "        print(f\"üìà Total de registros: {metadata['total_records']}\")\n",
        "        print(f\"‚úÖ Registros v√°lidos: {metadata['valid_records']}\")\n",
        "        print(f\"‚ùå Registros inv√°lidos: {metadata['invalid_records']}\")\n",
        "\n",
        "        # Top 10 pa√≠ses\n",
        "        print(\"\\nüåç TOP 10 PA√çSES M√ÅS POBLADOS\")\n",
        "        print(\"-\" * 80)\n",
        "        top_10 = etl.display_top_countries(10)\n",
        "        print(top_10.to_string(index=False))\n",
        "\n",
        "        # Estad√≠sticas descriptivas\n",
        "        print(\"\\nüìà ESTAD√çSTICAS DESCRIPTIVAS\")\n",
        "        print(\"-\" * 80)\n",
        "        stats = etl.get_summary_statistics()\n",
        "        print(stats)\n",
        "\n",
        "        # An√°lisis adicional\n",
        "        print(\"\\nüîç AN√ÅLISIS ADICIONAL\")\n",
        "        print(\"-\" * 80)\n",
        "        df = etl.transformed_data\n",
        "\n",
        "        print(f\"‚Ä¢ Poblaci√≥n total (top 50): {df['population_numeric'].sum():,.0f} habitantes\")\n",
        "        print(f\"‚Ä¢ Poblaci√≥n promedio: {df['population_numeric'].mean():,.0f} habitantes\")\n",
        "        print(f\"‚Ä¢ Pa√≠s m√°s poblado: {df.iloc[0]['country_clean']} ({df.iloc[0]['population_millions']:.2f}M)\")\n",
        "        print(f\"‚Ä¢ Distribuci√≥n por categor√≠a:\")\n",
        "        print(df['population_category'].value_counts().to_string())\n",
        "\n",
        "        # Porcentaje acumulado\n",
        "        top_10_percentage = df.head(10)['percentage_numeric'].sum()\n",
        "        print(f\"\\n‚Ä¢ Top 10 pa√≠ses representan: {top_10_percentage:.2f}% de la poblaci√≥n mundial\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"‚úÖ ETL COMPLETADO EXITOSAMENTE\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        return etl\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en el pipeline ETL: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "EmvGuksfC1Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# EJECUTAR\n",
        "# ============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    etl_pipeline = main()"
      ],
      "metadata": {
        "id": "sqxN4S_IC24j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}